{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Amazon SageMaker XGBoost algorithm\n",
    "_**Managed training for building a classification model with Amazon SageMaker XGBoost*_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMaker XGBoost to train and host a classification model. [XGBoost (eXtreme Gradient Boosting)](https://xgboost.readthedocs.io) is a popular and efficient machine learning algorithm used for regression and classification tasks on tabular datasets. It implements a technique know as gradient boosting on trees, and performs remarkably well in machine learning competitions, and gets a lot of attention from customers. \n",
    "\n",
    "We use the [MNIST data](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html) stored in [LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) format.\n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "This notebook was tested in Amazon SageMaker Studio on a ml.t3.medium instance with Python 3 (Data Science) kernel.\n",
    "\n",
    "Let's start by specifying:\n",
    "1. The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "1. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n",
      "CPU times: user 84.1 ms, sys: 2.15 ms, total: 86.3 ms\n",
      "Wall time: 185 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "### update below values appropriately ###\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "####\n",
    "\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the dataset\n",
    "\n",
    "Following code downloads the data and splits the data into train/validation datasets and upload files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import load_mnist, upload_to_s3\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"DEMO-smdebug-xgboost-mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to s3://sagemaker-us-east-1-365792799466/DEMO-smdebug-xgboost-mnist/train/mnist.train.libsvm\n",
      "Writing to s3://sagemaker-us-east-1-365792799466/DEMO-smdebug-xgboost-mnist/validation/mnist.validation.libsvm\n",
      "CPU times: user 3.74 s, sys: 420 ms, total: 4.16 s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_file, validation_file = load_mnist()\n",
    "upload_to_s3(train_file, bucket, f\"{prefix}/train/mnist.train.libsvm\")\n",
    "upload_to_s3(validation_file, bucket, f\"{prefix}/validation/mnist.validation.libsvm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the XGBoost model\n",
    "\n",
    "Now that we have the data uploaded to s3 we will use the XGBoost container to run our training.\n",
    "\n",
    "To run our training script on SageMaker, we construct a sagemaker.estimator.Estimator class, which accepts several constructor arguments:\n",
    "\n",
    "* __image_uri__: The path to the XG Boost Container that SageMaker runs for training and prediction.\n",
    "* __role__: Role ARN\n",
    "* __train_instance_type__ *(optional)*: The type of SageMaker instances for training. __Note__: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* __sagemaker_session__ *(optional)*: The session used to train on Sagemaker.\n",
    "* __hyperparameters__ *(optional)*: A dictionary passed to the train function as hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "# Below changes the region to be one where this notebook is running\n",
    "region = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "base_job_name = \"mnist-xgboost-classification\"\n",
    "bucket_path = \"s3://{}\".format(bucket)\n",
    "\n",
    "num_round = 50\n",
    "save_interval = 3\n",
    "hyperparameters = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.1\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"silent\": \"0\",\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": \"10\",  # num_class is required for 'multi:*' objectives\n",
    "    \"num_round\": num_round,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "xgboost_algorithm_mode_estimator = Estimator(\n",
    "    role=role,\n",
    "    base_job_name=base_job_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    image_uri=container,\n",
    "    hyperparameters=hyperparameters\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-11 09:45:05 Starting - Starting the training job...\n",
      "2021-08-11 09:45:30 Starting - Launching requested ML instancesProfilerReport-1628675105: InProgress\n",
      "...\n",
      "2021-08-11 09:46:04 Starting - Preparing the instances for training.........\n",
      "2021-08-11 09:47:36 Downloading - Downloading input data\n",
      "2021-08-11 09:47:36 Training - Downloading the training image....\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value multi:softmax to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[09:48:05] 48000x781 matrix with 7194988 entries loaded from /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[09:48:05] 12000x781 matrix with 1799168 entries loaded from /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mINFO:root:Distributed node training with 2 hosts: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34mINFO:RabitContextManager:Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[34mINFO:RabitContextManager:Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value multi:softmax to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[09:48:12] 48000x781 matrix with 7194988 entries loaded from /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[09:48:12] 12000x781 matrix with 1799168 entries loaded from /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mINFO:root:Distributed node training with 2 hosts: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:start listen on algo-1:9099\u001b[0m\n",
      "\u001b[34mINFO:RabitContextManager:Rabit slave environment: {'DMLC_TRACKER_URI': 'algo-1', 'DMLC_TRACKER_PORT': 9099}\u001b[0m\n",
      "\u001b[34mINFO:RabitContextManager:Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:No data received from connection ('10.2.91.238', 43794). Closing.\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:No data received from connection ('10.2.124.255', 41824). Closing.\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:Recieve start signal from 10.2.124.255; assign rank 0\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:Recieve start signal from 10.2.91.238; assign rank 1\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:@tracker All of 2 nodes getting started\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:@tracker All nodes finishes job\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:@tracker 0.172868013381958 secs between node start and job finish\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:start listen on algo-1:9100\u001b[0m\n",
      "\u001b[34mINFO:RabitContextManager:Rabit slave environment: {'DMLC_TRACKER_URI': 'algo-1', 'DMLC_TRACKER_PORT': 9100}\u001b[0m\n",
      "\u001b[34mINFO:RabitContextManager:Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:No data received from connection ('10.2.91.238', 58584). Closing.\u001b[0m\n",
      "\u001b[35mINFO:RabitContextManager:Connected to RabitTracker.\u001b[0m\n",
      "\u001b[35mINFO:RabitContextManager:Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[35mINFO:RabitContextManager:Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:No data received from connection ('10.2.124.255', 57470). Closing.\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:Recieve start signal from 10.2.124.255; assign rank 0\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:Recieve start signal from 10.2.91.238; assign rank 1\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 48000 rows\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:@tracker All of 2 nodes getting started\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 12000 rows\u001b[0m\n",
      "\u001b[34m[09:48:18] WARNING: /workspace/src/learner.cc:622: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[09:48:18] Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35mINFO:RabitContextManager:Connected to RabitTracker.\u001b[0m\n",
      "\u001b[35mINFO:root:Train matrix has 48000 rows\u001b[0m\n",
      "\u001b[35mINFO:root:Validation matrix has 12000 rows\u001b[0m\n",
      "\u001b[35m[09:48:18] WARNING: /workspace/src/learner.cc:622: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[09:48:18] Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\n",
      "2021-08-11 09:48:31 Training - Training image download completed. Training in progress.\u001b[35m[0] save global checkpoint #1 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #2 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #1 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #2 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[0]#011train-merror:0.174979#011validation-merror:0.187333\u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #3 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #4 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #3 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[1]#011train-merror:0.141833#011validation-merror:0.154083\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #4 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #5 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #6 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #5 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #6 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[2]#011train-merror:0.120271#011validation-merror:0.134333\u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #7 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #8 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #7 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[3]#011train-merror:0.112938#011validation-merror:0.126417\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #8 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #9 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[4]#011train-merror:0.108375#011validation-merror:0.1215\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #10 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #9 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #10 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #11 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[5]#011train-merror:0.103583#011validation-merror:0.115917\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #12 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #11 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #12 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #13 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[6]#011train-merror:0.099021#011validation-merror:0.110417\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #14 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #13 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #14 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #15 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[7]#011train-merror:0.094917#011validation-merror:0.107833\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #16 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #15 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #16 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #17 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[8]#011train-merror:0.090792#011validation-merror:0.104083\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #18 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #17 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #18 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #19 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[9]#011train-merror:0.088208#011validation-merror:0.101667\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #20 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #19 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #20 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #21 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[10]#011train-merror:0.085396#011validation-merror:0.0985\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #22 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #21 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #22 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #23 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[11]#011train-merror:0.082375#011validation-merror:0.09575\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #24 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #23 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #24 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #25 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[12]#011train-merror:0.08#011validation-merror:0.093333\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #26 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #25 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #26 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #27 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[13]#011train-merror:0.077188#011validation-merror:0.090083\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #28 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #27 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #28 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #29 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[14]#011train-merror:0.075438#011validation-merror:0.088167\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #30 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #29 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #30 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #31 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[15]#011train-merror:0.073375#011validation-merror:0.087\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #32 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #31 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #32 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #33 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #34 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #33 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[16]#011train-merror:0.071771#011validation-merror:0.08575\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #34 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #35 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[17]#011train-merror:0.069917#011validation-merror:0.08425\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #36 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #35 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #36 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #37 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #38 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #37 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[18]#011train-merror:0.067792#011validation-merror:0.082583\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #38 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #39 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[19]#011train-merror:0.066333#011validation-merror:0.082083\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #40 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #39 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #40 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #41 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #42 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #41 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[20]#011train-merror:0.064417#011validation-merror:0.07975\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #42 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #43 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[21]#011train-merror:0.063208#011validation-merror:0.07825\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #44 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #43 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #44 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #45 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #46 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #45 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[22]#011train-merror:0.061458#011validation-merror:0.076333\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #46 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #47 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #48 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #47 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[23]#011train-merror:0.060292#011validation-merror:0.075917\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #48 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #49 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #50 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[24]#011train-merror:0.059146#011validation-merror:0.074583\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #51 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[25]#011train-merror:0.057583#011validation-merror:0.073333\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #52 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #49 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #50 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #51 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #52 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #53 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #54 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #53 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[26]#011train-merror:0.056313#011validation-merror:0.072083\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #54 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #55 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #56 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #55 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[27]#011train-merror:0.055188#011validation-merror:0.070083\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #56 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #57 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[28]#011train-merror:0.054042#011validation-merror:0.069083\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #58 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #57 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #58 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #59 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #60 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #59 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #60 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[29]#011train-merror:0.052646#011validation-merror:0.06825\u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #61 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #62 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #61 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[30]#011train-merror:0.051625#011validation-merror:0.066833\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #62 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #63 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[31]#011train-merror:0.050521#011validation-merror:0.066333\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #64 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #63 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #64 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #65 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #66 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #65 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[32]#011train-merror:0.049375#011validation-merror:0.0655\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #66 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #67 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[33]#011train-merror:0.048625#011validation-merror:0.064583\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #68 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #67 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #68 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #69 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #70 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #69 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[34]#011train-merror:0.047125#011validation-merror:0.063083\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #70 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #71 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #72 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #71 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[35]#011train-merror:0.045833#011validation-merror:0.06225\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #72 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #73 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #74 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #73 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[36]#011train-merror:0.04475#011validation-merror:0.061417\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #74 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #75 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #76 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #75 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[37]#011train-merror:0.043875#011validation-merror:0.06025\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #76 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #77 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #78 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #77 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[38]#011train-merror:0.042875#011validation-merror:0.0595\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #78 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #79 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #79 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[39]#011train-merror:0.041563#011validation-merror:0.058417\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #80 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #80 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #81 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #82 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #81 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[40]#011train-merror:0.040625#011validation-merror:0.058083\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #82 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #83 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #84 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #83 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[41]#011train-merror:0.040042#011validation-merror:0.057083\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #84 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #85 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[42]#011train-merror:0.039083#011validation-merror:0.05675\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #86 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #85 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #86 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #87 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #88 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #87 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[43]#011train-merror:0.038104#011validation-merror:0.056\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #88 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #89 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #90 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #89 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[44]#011train-merror:0.037458#011validation-merror:0.0555\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #90 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #91 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #92 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #91 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[45]#011train-merror:0.036438#011validation-merror:0.054917\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #92 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #93 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #94 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #93 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[46]#011train-merror:0.035792#011validation-merror:0.054083\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #94 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #95 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[47]#011train-merror:0.03525#011validation-merror:0.053833\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #96 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #95 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #96 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #97 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #98 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #97 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[48]#011train-merror:0.034562#011validation-merror:0.05325\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #98 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #99 \u001b[0m\n",
      "\u001b[35m[0] save global checkpoint #100 \u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #99 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:[49]#011train-merror:0.033729#011validation-merror:0.0525\u001b[0m\n",
      "\u001b[34m[1] save global checkpoint #100 \u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:@tracker All nodes finishes job\u001b[0m\n",
      "\u001b[34mINFO:RabitTracker:@tracker 232.25054001808167 secs between node start and job finish\u001b[0m\n",
      "\n",
      "2021-08-11 09:53:12 Uploading - Uploading generated training model\n",
      "2021-08-11 09:53:32 Completed - Training job completed\n",
      "ProfilerReport-1628675105: NoIssuesFound\n",
      "Training seconds: 718\n",
      "Billable seconds: 718\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.session import TrainingInput\n",
    "\n",
    "train_s3_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"train\"), content_type=\"libsvm\"\n",
    ")\n",
    "validation_s3_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"validation\"), content_type=\"libsvm\"\n",
    ")\n",
    "\n",
    "\n",
    "xgboost_algorithm_mode_estimator.fit(\n",
    "    {\"train\": train_s3_input, \"validation\": validation_s3_input}, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost Estimator on MNIST data using Spot Instance\n",
    "\n",
    "Training using spot instance is just adding a simple configuration attribute 'use_spot_instances' to the estimator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Source distributed script mode\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "session = Session(boto_session=boto_session)\n",
    "\n",
    "xgboost_algorithm_mode_estimator = Estimator(\n",
    "    role=role,\n",
    "    base_job_name=base_job_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    image_uri=container,\n",
    "    hyperparameters=hyperparameters,\n",
    "    use_spot_instances=True,\n",
    "    max_run=3600,\n",
    "    max_wait=3600\n",
    "    \n",
    ")\n",
    "\n",
    "train_s3_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"train\"), content_type=\"libsvm\"\n",
    ")\n",
    "validation_s3_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"validation\"), content_type=\"libsvm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-11 10:32:52 Starting - Starting the training job...\n",
      "2021-08-11 10:33:15 Starting - Launching requested ML instancesProfilerReport-1628677972: InProgress\n",
      "...\n",
      "2021-08-11 10:33:47 Starting - Preparing the instances for training............\n",
      "2021-08-11 10:35:45 Downloading - Downloading input data...\n",
      "2021-08-11 10:36:16 Training - Training image download completed. Training in progress.....\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value multi:softmax to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[10:36:14] 48000x781 matrix with 7194988 entries loaded from /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[10:36:14] 12000x781 matrix with 1799168 entries loaded from /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 48000 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 12000 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-merror:0.177167#011validation-merror:0.186917\u001b[0m\n",
      "\u001b[34m[1]#011train-merror:0.139938#011validation-merror:0.156833\u001b[0m\n",
      "\u001b[34m[2]#011train-merror:0.124042#011validation-merror:0.137667\u001b[0m\n",
      "\u001b[34m[3]#011train-merror:0.113021#011validation-merror:0.126083\u001b[0m\n",
      "\u001b[34m[4]#011train-merror:0.107854#011validation-merror:0.120833\u001b[0m\n",
      "\u001b[34m[5]#011train-merror:0.102646#011validation-merror:0.1165\u001b[0m\n",
      "\u001b[34m[6]#011train-merror:0.098771#011validation-merror:0.111417\u001b[0m\n",
      "\u001b[34m[7]#011train-merror:0.09425#011validation-merror:0.107417\u001b[0m\n",
      "\u001b[34m[8]#011train-merror:0.091604#011validation-merror:0.103417\u001b[0m\n",
      "\u001b[34m[9]#011train-merror:0.088292#011validation-merror:0.10125\u001b[0m\n",
      "\u001b[34m[10]#011train-merror:0.084396#011validation-merror:0.099167\u001b[0m\n",
      "\u001b[34m[11]#011train-merror:0.082646#011validation-merror:0.09625\u001b[0m\n",
      "\u001b[34m[12]#011train-merror:0.079938#011validation-merror:0.094\u001b[0m\n",
      "\u001b[34m[13]#011train-merror:0.0775#011validation-merror:0.092333\u001b[0m\n",
      "\u001b[34m[14]#011train-merror:0.075208#011validation-merror:0.091\u001b[0m\n",
      "\u001b[34m[15]#011train-merror:0.072875#011validation-merror:0.08875\u001b[0m\n",
      "\u001b[34m[16]#011train-merror:0.071417#011validation-merror:0.087083\u001b[0m\n",
      "\u001b[34m[17]#011train-merror:0.069708#011validation-merror:0.085667\u001b[0m\n",
      "\u001b[34m[18]#011train-merror:0.067729#011validation-merror:0.0835\u001b[0m\n",
      "\u001b[34m[19]#011train-merror:0.066417#011validation-merror:0.082333\u001b[0m\n",
      "\u001b[34m[20]#011train-merror:0.064875#011validation-merror:0.080833\u001b[0m\n",
      "\u001b[34m[21]#011train-merror:0.063896#011validation-merror:0.079917\u001b[0m\n",
      "\u001b[34m[22]#011train-merror:0.062417#011validation-merror:0.078833\u001b[0m\n",
      "\u001b[34m[23]#011train-merror:0.060958#011validation-merror:0.077833\u001b[0m\n",
      "\u001b[34m[24]#011train-merror:0.059792#011validation-merror:0.076333\u001b[0m\n",
      "\u001b[34m[25]#011train-merror:0.0585#011validation-merror:0.075083\u001b[0m\n",
      "\u001b[34m[26]#011train-merror:0.05725#011validation-merror:0.074333\u001b[0m\n",
      "\u001b[34m[27]#011train-merror:0.055729#011validation-merror:0.071833\u001b[0m\n",
      "\u001b[34m[28]#011train-merror:0.054604#011validation-merror:0.071333\u001b[0m\n",
      "\u001b[34m[29]#011train-merror:0.0535#011validation-merror:0.070333\u001b[0m\n",
      "\u001b[34m[30]#011train-merror:0.052396#011validation-merror:0.06875\u001b[0m\n",
      "\u001b[34m[31]#011train-merror:0.051354#011validation-merror:0.067833\u001b[0m\n",
      "\u001b[34m[32]#011train-merror:0.050167#011validation-merror:0.066833\u001b[0m\n",
      "\u001b[34m[33]#011train-merror:0.0495#011validation-merror:0.06625\u001b[0m\n",
      "\u001b[34m[34]#011train-merror:0.048229#011validation-merror:0.065333\u001b[0m\n",
      "\u001b[34m[35]#011train-merror:0.047438#011validation-merror:0.0645\u001b[0m\n",
      "\u001b[34m[36]#011train-merror:0.046188#011validation-merror:0.063833\u001b[0m\n",
      "\u001b[34m[37]#011train-merror:0.045333#011validation-merror:0.06275\u001b[0m\n",
      "\u001b[34m[38]#011train-merror:0.0445#011validation-merror:0.061667\u001b[0m\n",
      "\u001b[34m[39]#011train-merror:0.043646#011validation-merror:0.061333\u001b[0m\n",
      "\u001b[34m[40]#011train-merror:0.042729#011validation-merror:0.060083\u001b[0m\n",
      "\u001b[34m[41]#011train-merror:0.041896#011validation-merror:0.059\u001b[0m\n",
      "\u001b[34m[42]#011train-merror:0.041229#011validation-merror:0.058583\u001b[0m\n",
      "\u001b[34m[43]#011train-merror:0.040188#011validation-merror:0.058583\u001b[0m\n",
      "\u001b[34m[44]#011train-merror:0.039417#011validation-merror:0.057333\u001b[0m\n",
      "\u001b[34m[45]#011train-merror:0.038542#011validation-merror:0.056417\u001b[0m\n",
      "\u001b[34m[46]#011train-merror:0.037979#011validation-merror:0.05525\u001b[0m\n",
      "\u001b[34m[47]#011train-merror:0.037417#011validation-merror:0.055\u001b[0m\n",
      "\u001b[34m[48]#011train-merror:0.036438#011validation-merror:0.054583\u001b[0m\n",
      "\u001b[34m[49]#011train-merror:0.035729#011validation-merror:0.054083\u001b[0m\n",
      "\n",
      "2021-08-11 10:38:17 Uploading - Uploading generated training model\n",
      "2021-08-11 10:38:17 Completed - Training job completed\n",
      "Training seconds: 145\n",
      "Billable seconds: 54\n",
      "Managed Spot Training savings: 62.8%\n"
     ]
    }
   ],
   "source": [
    "xgboost_algorithm_mode_estimator.fit(\n",
    "    {\"train\": train_s3_input, \"validation\": validation_s3_input}, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with XGBoost Estimator on MNIST data using Spot Instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared the dataset and trained our model, one thing to note is there are algorithm settings which are called \"hyperparameters\" that can dramtically affect the performance of the trained models. For example, XGBoost algorithm has dozens of hyperparameters and we need to pick the right values for those hyperparameters in order to achieve the desired model training results. Since which hyperparameter setting can lead to the best result depends on the dataset as well, it is almost impossible to pick the best hyperparameter setting without searching for it, and a good search algorithm can search for the best hyperparameter setting in an automated and effective way.\n",
    "\n",
    "We will use SageMaker hyperparameter tuning to automate the searching process effectively. Specifically, we specify a range, or a list of possible values in the case of categorical hyperparameters, for each of the hyperparameter that we plan to tune. SageMaker hyperparameter tuning will automatically launch multiple training jobs with different hyperparameter settings, evaluate results of those training jobs based on a predefined \"objective metric\", and select the hyperparameter settings for future attempts based on previous results. For each hyperparameter tuning job, we will give it a budget (max number of training jobs) and it will complete once that many training jobs have been executed.\n",
    "\n",
    "\n",
    "Now we configure the hyperparameter tuning job by using the SDK that specifies following information:\n",
    "* The Estimator to use for HPO. This we created in the earlier step for training.\n",
    "* The ranges of hyperparameters we want to tune\n",
    "* Number of training jobs to run in total and how many training jobs should be run simultaneously. More parallel jobs will finish tuning sooner, but may sacrifice accuracy. We recommend you set the parallel jobs value to less than 10% of the total number of training jobs (we'll set it higher just for this example to keep it short).\n",
    "* The objective metric that will be used to evaluate training results, in this example, we select *validation:auc* to be the objective metric and the goal is to maximize the value throughout the hyperparameter tuning process. One thing to note is the objective metric has to be among the metrics that are emitted by the algorithm during training. In this example, the built-in XGBoost algorithm emits a bunch of metrics and *validation:auc* is one of them. If you bring your own algorithm to SageMaker, then you need to make sure whatever objective metric you select, your algorithm actually emits it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tune three hyperparameters in this examples:\n",
    "* *eta*: Step size shrinkage used in updates to prevent overfitting. After each boosting step, you can directly get the weights of new features. The eta parameter actually shrinks the feature weights to make the boosting process more conservative.  \n",
    "* *min_child_weight*: Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, the building process gives up further partitioning. In linear regression models, this simply corresponds to a minimum number of instances needed in each node. The larger the algorithm, the more conservative it is. \n",
    "* *max_depth*: Maximum depth of a tree. Increasing this value makes the model more complex and likely to be overfitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "# Define hyperparameter ranges.\n",
    "hyperparameter_ranges = {\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"min_child_weight\": ContinuousParameter(1, 10),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll specify the objective metric that we'd like to tune and its definition, which includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of the training job. Since we are using built-in XGBoost algorithm here, it emits two predefined metrics: *validation:auc* and *train:auc*, and we elected to monitor *validation:auc* as you can see below. In this case, we only need to specify the metric name and do not need to provide regex. If you bring your own algorithm, your algorithm emits metrics by itself. In that case, you'll need to add a MetricDefinition object here to define the format of those metrics through regex, so that SageMaker knows how to extract those metrics from your CloudWatch logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = \"validation:merror\"\n",
    "objective_type = 'Minimize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a `HyperparameterTuner` object, to which we pass:\n",
    "- The XGBoost estimator we created above\n",
    "- Our hyperparameter ranges\n",
    "- Objective metric name and definition\n",
    "- Tuning resource configurations such as Number of training jobs to run in total and how many training jobs can be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    xgboost_algorithm_mode_estimator, objective_metric_name, hyperparameter_ranges, max_jobs=2, max_parallel_jobs=2,objective_type=objective_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch_Hyperparameter_Tuning\n",
    "Now we can launch a hyperparameter tuning job by calling *fit()* function. After the hyperparameter tuning job is created, we can go to SageMaker console to track the progress of the hyperparameter tuning job until it is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................."
     ]
    }
   ],
   "source": [
    "train_s3_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"train\"), content_type=\"libsvm\"\n",
    ")\n",
    "validation_s3_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"validation\"), content_type=\"libsvm\"\n",
    ")\n",
    "\n",
    "tuner.fit(\n",
    "    {\"train\": train_s3_input, \"validation\": validation_s3_input}, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Distributed training\n",
    "\n",
    "SageMaker's XGBoost Algorithm supports distributed training by default. We just need to increase the instance_count(number of instances) while creating the estimator.\n",
    " \n",
    "```python\n",
    "\n",
    "xgboost_algorithm_mode_estimator = Estimator(\n",
    "    role=role,\n",
    "    base_job_name=base_job_name,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    image_uri=container,\n",
    "    hyperparameters=hyperparameters\n",
    "    \n",
    ")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
